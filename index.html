<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>reveal.js - The HTML Presentation Framework</title>

		<meta name="description" content="A framework for easily creating beautiful presentations using HTML">
		<meta name="author" content="Hakim El Hattab">

		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="css/reveal.min.css">
		<link rel="stylesheet" href="css/theme/default.css" id="theme">

		<!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/zenburn.css">

        <!-- For syntax highlighting -->
        <!-- <link rel="stylesheet" href="bower_components/reveal-highlight-themes/styles/tomorrow.css" id="highlight-theme"> -->

		<!-- If the query includes 'print-pdf', include the PDF print sheet -->
		<script>
			if( window.location.search.match( /print-pdf/gi ) ) {
				var link = document.createElement( 'link' );
				link.rel = 'stylesheet';
				link.type = 'text/css';
				link.href = 'css/print/pdf.css';
				document.getElementsByTagName( 'head' )[0].appendChild( link );
			}
		</script>

		<!--[if lt IE 9]>
		<script src="lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<!-- Any section element inside of this container is displayed as a slide -->
			<div class="slides">
				<section>
					<h1>Intro to Machine Learning</h1>
					<p>
                    <small>I'm Jamis</small>
                    </p>
                    <p>
                    <small><a target="_blank" href="http://jamiis.me">jamiis.me</a> | <a target="_blank" href="http://github.com/jamiis">github.com/jamiis</a> | <a href="http://twitter.com/jamisjohnson">@jamisjohnson</a></small>
					</p>
				</section>

                <section>
                    <h2>Goals</h2>
                    <ul>
                        <li>Fast, brief, <strong>light</strong> intro to supervised learning</li>
                        <li>Meant to pique interest, not to give an in-depth understanding</li>
                        <li>Half math, half code</li>
                    </ul>
                </section>

                <section>
                    <h2>Download scikit-learn</h2>
                    <pre><code data-trim contenteditable>
$ pip install -U pip
$ pip install -U numpy scipy scikit-learn
                    </code></pre>
                </section>

                <section>
                    <h2>The 3 Categories of ML</h2>
                    <ul>
                        <li><strong>Supervised Learning</strong>: Given data composed of inputs corresponding to outputs, learn a general rule for the mapping.</li>
                        <li><strong>Unsupervised Learning</strong>: We are given inputs but the outputs are unknown and it is up to us to discover structure.</li>
                        <li><strong>Reinforcement Learning</strong>: Interact with a dynamic environment to achieve some goal.</li>
                    </ul>
                </section>

                <section>
                    <h2>Supervised Learning</h2>
                    <img src="lib/img/ml.png" height="300px">
                    <blockquote cite="http://cs229.stanford.edu/notes/cs229-notes1.pdf">
When the target variable that weâ€™re trying to predict is continuous,
we call the learning problem a regression problem.
When y can take on only a small number of discrete values,
we call it a classification problem.
                    </blockquote>
                </section>

                <section>
                    <h2>1D Regression Example Data</h2>
                    Model Price as a function of Area
                    <br>
                    \[ f(Area) = Price \]
                    <img src="lib/img/table1.png" height="500px">
                </section>

                <section>
                    <h2>Input Data</h2>
                    <img src="lib/img/scatter.png" height="500px">
                </section>

                <section>
                    <h2>Model with a Line</h2>
                    <img src="lib/img/scatter2.png" height="500px">
                </section>

                <section>
                    <h2>Hypothesis: a Line!</h2>
                    A line provides a good fit to the input data

                    <p class="fragment">
                    <br>
                    Remember me? The equation of a line!
                    \[ y = mx + b \]

                    <p class="fragment">
                    <br>
                    Re-written with different syntax ...
                    \[ h(x) = \theta_1 x + \theta_0 \]

                    <p class="fragment">
                    <br>
                    Re-arranged ...
                    \[ h(x) = \theta_0 + \theta_1 x \]

                    <p class="fragment">
                    <br>
                    \( \theta \)s are called the <em>parameters</em> or <em>weights</em> of our model
                </section>

                <section>
                    <h2>Supervised Learning</h2>
                    <img src="lib/img/flow.png" height="500px">
                </section>

                <section>
                    <h2>2D Regression Example</h2>
                    Model Price as a function of both Area <em>and</em> Bedrooms
                    <br>
                    \[ f(Area,Bedrooms) = Price \]
                    <img src="lib/img/table2.png">
                </section>

                <section>
                    <h2>Model with a plane!</h2>
                    <img src="lib/img/plane.png" height="500px">
                </section>

                <section>
                    <h2>What about more dimensions?</h2>
                </section>

                <section>
                    <h2>Hypothesis: a Hyperplane!</h2>
                    <p class="fragment">
                    <br>
                    A Line ... 
                    \[ h(x) = \theta_0 + \theta_1 x \]

                    <p class="fragment">
                    <br>
                    A plane ... 
                    \[ h(x) = \theta_0 + \theta_1 x + \theta_2 x \]

                    <p class="fragment">
                    <br>
                    A hyperplane!
                    \[ h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 \dots \theta_n x_n \]

                    <p class="fragment">
                    <br>
                    Let \(x_0 = 1\) and condense. Don't let the linear algebra scare you!
                    \[ h(x) = \sum_{i=0}^n \theta_i x_i = \theta^T x\]
                </section>

                <section>
                    <h2>Least Squares Cost Function</h2>
                    \[
J(\theta) = \frac{1}{2} \sum_{i=0}^m (h_{\theta}(x^{(i)}) - y^{(i)})^2
                    \]
<p>
<br>
Simply quantifies the total error between all input 
data points and the hypothesis function.
</p>
<p>
<br>
If we minimize this error we'll have the optimal weights \(\theta\)!
</p>
                </section>

                <section>
                    <h2>Find Best Fit: Gradient Descent</h2>
                    Minimize error between the model and data:
                    \[
\theta_j = \theta_j - 
\alpha \frac{\partial}{\partial \theta_j} J(\theta)
                    \]
                    <a href="http://nl-hugo.github.io/d3-gradient-descent/index.html" target="_blank">
                    <img src="lib/img/grad.gif" >
                    </a>
                </section>

                <section>
                    <h2>Supervised Learning</h2>
                    <p>
Learning is merely minimizing error between our hypothesis function and 
the training set. We can then make predictions using \(h\).
                    </p>
                    <img src="lib/img/flow.png" height="500px">
                </section>

                <section>
                    <h2>Linear Regression</h2>
                    <pre><code data-trim contenteditable>
>>> from sklearn import linear_model
>>> classifier = linear_model.LinearRegression()
>>> X = [[0], [1], [2]]
>>> y = [ 0,   2,   4]
>>> classifier.fit(X,y)
LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
>>> classifier.coef_
array([ 2.])
>>> new_data = [[3],[4]]
>>> classifier.predict(new_data)
array([ 6.,  8.])
                    </code></pre>
                </section>

                <section>
                    <h2>Polynomial Regression</h2>
                    \[
h(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 +
\theta_3 x_1 x_2 + \theta_4 x_1^2 + \theta_5 x_2^2 ...
                    \]
                    <img src="lib/img/poly.png" height="500px">
                </section>

                <section>
                    <h2>Problem: Overfitting</h2>
<p>
As model complexity increases, accuracy of approximating the ground truth decreases.
</p>
                    <img src="lib/img/overfit2.png" >
                </section>

                <section>
                    <h2>Solution: Regularization</h2>
                    <p>
Penalize extreme parameter values to reduce model complexity.
                    </p>
                    <p>
                    Well Known: Lasso, Ridge, ElasticNet
                    </p>
                    <img src="lib/img/reg_coeff.png" height="500px">
                </section>

                <section>
                    <h2>Regularized</h2>
                    <h2>Polynomial Regression</h2>
                    <pre><code data-trim contenteditable>
print(__doc__)

# Author: Mathieu Blondel
#         Jake Vanderplas
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import Ridge
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline

def f(x):
    """ function to approximate by polynomial interpolation"""
    return x * np.sin(x)

# generate points used to plot
x_plot = np.linspace(0, 10, 100)

# generate points and keep a subset of them
x = np.linspace(0, 10, 100)
rng = np.random.RandomState(0)
rng.shuffle(x)
x = np.sort(x[:20])
y = f(x)

# create matrix versions of these arrays
X = x[:, np.newaxis]
X_plot = x_plot[:, np.newaxis]

plt.plot(x_plot, f(x_plot), label="ground truth")
plt.scatter(x, y, label="training points")

for degree in [3, 4, 5]:
    model = make_pipeline(PolynomialFeatures(degree), Ridge())
    model.fit(X, y)
    y_plot = model.predict(X_plot)
    plt.plot(x_plot, y_plot, label="degree %d" % degree)

plt.legend(loc='lower left')
plt.show()
                    </code></pre>
                </section>

                <section>
                    <h2>Model Assessment:</h2>
                    <h2>Cross Validation</h2>
                    <p>
                    Split data into k "folds"; train model then assess error on validation set (\(\frac{1}{k}\) of the data); average error across all sets
                    </p>
                    <img src="lib/img/kfold.png" >
                </section>

                <section>
                    <h2>Cross Validation</h2>
                    <img src="lib/img/crossfit.png" height="500px">
                </section>

                <section>
                    <h2>iPython Data Exploration</h2>
                    ... cut to terminal
                    <p class="fragment">
                    <br>
                    <a href="http://www.datarobot.com/blog/gradient-boosted-regression-trees/" target="_blank">
                        Full California Housing Example using Gradient Boosted Regression Trees
                    </a>
                    </p>
                </section>

                <section>
                    <h2>Classification</h2>
                    <ul>
                        <li>Logistic Regression</li>
                        <li>
                            <a href="http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html#support-vector-machines-svms" target="_blank">
                                Support Vector Machines
                            </a>
                        </li>
                        <li>Perceptrons</li>
                        <li>
                            <a href="http://scikit-learn.org/stable/auto_examples/classification/plot_lda_qda.html" target="_blank">
                                Linear Discriminant Analysis
                            </a>
                        </li>

                    </ul>
                    <img src="lib/img/classification.jpg">
                </section>

                <section>
                    <h2>Mmmm Machine Learning</h2>
                    <ul>
                        <li>
                            <a href="http://scikit-learn.org/stable/user_guide.html" target="_blank">
                                scikit-learn Tutorials
                            </a>
                        <li>
                    <a href="https://www.coursera.org/learn/machine-learning" target="_blank">
                        Stanford Coursera
                    </a>
                        <li>Machine Learning at Scale:
                            <ul>
                                <li>
                                    <a href="https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x" target="_blank">
                                        Spark
                                    </a>
                                </li>
                                <li>Hadoop too but meh :)</li>
                            </ul>
                        </li>
                        <li>Neural Net Libraries:
                            <ul>
                                <li>
                                    <a href="http://karpathy.github.io/neuralnets/" target="_blank">
                                        <em>Hacker's Guide to Neural Networks</em>
                                    </a>
                                    (Andrej Karpathy is amazing)
                                </li>
                                <li>Theano (Python, Symbolic Syntax, University of Montreal)</li>
                                <li>Torch (Lua, NYU, DeepMind, FaceBook)</li>
                                <li>Caffe (C++/Python interface, Berkeley, Google DeepDream)</li>
                            </ul>
                        </li>
                    </ul>
                </section>

                <section>
                    <h2>Thanks!</h2>
                </section>

			</div>

		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.min.js"></script>
        <!-- MathJax for rendering LaTex-->
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>


		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			Reveal.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: Reveal.getQueryHash().theme || 'night', // available themes are in /css/theme
				transition: Reveal.getQueryHash().transition || 'none', // default/cube/page/concave/zoom/linear/fade/none

				// Parallax scrolling
                parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
                parallaxBackgroundSize: '2100px 900px',

				// Optional libraries used to extend on reveal.js
				dependencies: [
					{ src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
					{ src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
					{ src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
					{ src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
				]
			});

		</script>

	</body>
</html>
